{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RA_duplicate_counterexample.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg9QHee3zq5u"
      },
      "source": [
        "**Get data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn4cpatEYMei"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMhmbNvCaljd"
      },
      "source": [
        "# cd drive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE70xX4Wz1Sk"
      },
      "source": [
        "**Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xThw-sYUatPh"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, InputLayer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwKGExz2z5jN"
      },
      "source": [
        "**Upsample/duplicate counterexamples**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pifum1sx2_jU"
      },
      "source": [
        "*Technique 1*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW4gwzNUavDk"
      },
      "source": [
        "def upsample(eps_df, upsample_to = 10):\n",
        "\n",
        "  cnt = 0\n",
        "  upsample_lst = []\n",
        "  \n",
        "  # Get feature data\n",
        "  counter_x = eps_df.iloc[:,2:27].to_numpy()\n",
        "  counter_y = eps_df.iloc[:,27:].to_numpy()\n",
        "  print(\"Number of counter examples:\", counter_x.shape[0])\n",
        "\n",
        "  # Extract epsilons from counterexample data\n",
        "  eps = eps_df.to_numpy()[:,1]\n",
        "\n",
        "  # Upsample counterexamples \n",
        "  # Technique 1: Upsample counterexamples by choosing a random number between \n",
        "  # [epsilon, epsilon + 0.001] and adding and subtracting it to one counterexample.\n",
        "  # Repeat this procedure n times to generate 2n more examples per counterexample.\n",
        "\n",
        "  for i,j in zip(counter_x, counter_y):\n",
        "    \n",
        "    # Get epsilon for the counterexample\n",
        "    epsilon = eps[cnt]\n",
        "\n",
        "    # Upsampling for one counterexample\n",
        "    for n in range(upsample_to):\n",
        "\n",
        "      # Generate random number\n",
        "      r = random.uniform(epsilon, epsilon+0.001)\n",
        "      # Add random number to counterexample\n",
        "      i_new_high = i + r\n",
        "      # Subtract random number from counterexample\n",
        "      i_new_low = i - r\n",
        "      # Append the newly formed feature vectors with the target vector\n",
        "      ij_new_high = np.hstack((i_new_high, j))\n",
        "      ij_new_low = np.hstack((i_new_low, j))\n",
        "    \n",
        "      upsample_lst.append(ij_new_high)\n",
        "      upsample_lst.append(ij_new_low)\n",
        "\n",
        "    cnt += 1\n",
        "\n",
        "  print(\"Number of upsampled counterexamples:\",len(upsample_lst))\n",
        "  return np.asarray(upsample_lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IRtL4hh4Pfu"
      },
      "source": [
        "*Technique 2*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOgxOSYd2yy5"
      },
      "source": [
        "def upsample_duplicate(eps_df, upsample_to = 10):\n",
        "\n",
        "  cnt = 0\n",
        "  upsample_lst = []\n",
        "  \n",
        "  # Get feature data\n",
        "  counter_x = eps_df.iloc[:,2:27].to_numpy()\n",
        "  counter_y = eps_df.iloc[:,27:].to_numpy()\n",
        "  print(\"Number of counter examples:\", counter_x.shape[0])\n",
        "\n",
        "  # Extract epsilons from counterexample data\n",
        "  eps = eps_df.to_numpy()[:,1]\n",
        "\n",
        "  # Upsample counterexamples \n",
        "  # Technique 2: Duplicate every counterexample n times to generate n more examples.\n",
        "\n",
        "  for i,j in zip(counter_x, counter_y):\n",
        "\n",
        "    # Get epsilon for the counterexample\n",
        "    epsilon = eps[cnt]\n",
        "\n",
        "    # Upsampling for one counterexample\n",
        "    for n in range(upsample_to):\n",
        "      ij_new = np.hstack((i, j))\n",
        "      upsample_lst.append(ij_new)\n",
        "    cnt += 1\n",
        "\n",
        "  print(\"Number of upsampled counterexamples:\",len(upsample_lst))\n",
        "  return np.asarray(upsample_lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpzVV96ez-nJ"
      },
      "source": [
        "**Train function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "antd9ynDclU8"
      },
      "source": [
        "def train(X_train, X_test, y_train, y_test, epochs = 30, batch_size = 16):\n",
        "\n",
        "\n",
        "  # Define model architecture\n",
        "  model = Sequential()\n",
        "  model.add(InputLayer(input_shape=(X_train.shape[1],)))\n",
        "  model.add(Dense(23, activation='relu', kernel_initializer='he_normal'))\n",
        "  model.add(Dense(18, activation='relu'))\n",
        "  model.add(Dense(11, activation='relu'))\n",
        "  model.add(Dense(5, activation='softmax')) # logits layer\n",
        "\n",
        "  # Define optimizer\n",
        "  optimizer = 'adam'\n",
        "  # Define crtierion and compile\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  # Save checkpoint\n",
        "  checkpoint_path = \"cou_ex_model_duplicate.ckpt\"\n",
        "  checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "  # Create a callback that saves the best model based on validation accuracy\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                  save_best_only=True, monitor='val_accuracy', save_weights_only = False,\n",
        "                                                  verbose=1)\n",
        "  \n",
        "  # Train\n",
        "  history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=[cp_callback])\n",
        "  \n",
        "  # Save model in .pb and .h5 format\n",
        "  model.save('./duplicate_counterex_model')\n",
        "  model.save('./duplicate_counterex_model.h5', save_format='h5')\n",
        "  \n",
        "  return history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1B-AH_z0CT6"
      },
      "source": [
        "**Plot loss and accuracy curves**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPUAPZBXeSC7"
      },
      "source": [
        "def plot_acc(history):\n",
        "  # summarize history for accuracy\n",
        "\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaJyP0VYefop"
      },
      "source": [
        "def plot_loss(history):\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKZ9cEbu0F0g"
      },
      "source": [
        "**Main function | Read data, upsample counterexample data, train and evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhaSKKgs5VJe"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Read data\n",
        "  print(\"~ Reading data...\")\n",
        "  df_counter_784 = pd.read_csv(\"counterexamples_adversarial.csv\")\n",
        "  df_counter_784_eps = pd.read_csv(\"epsilon_counterexamples.csv\")\n",
        "  df_org_trainfile = pd.read_csv(\"v3.2.2_train.csv\")\n",
        "  df_counter_2000 = pd.read_csv(\"counterexamples2000.csv\")\n",
        "  df_counter_2000_eps = pd.read_csv(\"epsilon_counterexamples2000.csv\")\n",
        "  df_counter_clustering = pd.read_csv(\"region_counterexamples_clustering.csv\")\n",
        "  df_counter_clus_eps_4000 = pd.read_csv(\"eps_counter_clustering4000.csv\")\n",
        "\n",
        "  print(\"~ Done\")\n",
        "\n",
        "  print(\"-\"*30)\n",
        "  print(\"~ Preprocessing...\")\n",
        "  \n",
        "  counterex_robust = df_counter_2000_eps.iloc[:,2:].to_numpy()\n",
        "  counterex_clus = df_counter_clus_eps_4000.iloc[:, 1:].to_numpy()\n",
        "\n",
        "  # Upsample required data\n",
        "  upsample_counterex_robust = upsample(df_counter_2000_eps.iloc[:,1:], upsample_to = 100)\n",
        "\n",
        "  upsample_counterex_clus = upsample(df_counter_clus_eps_4000, upsample_to = 100)\n",
        "\n",
        "\n",
        "  # Original data\n",
        "  org_train_np = df_org_trainfile.iloc[:,1:].to_numpy()\n",
        "\n",
        "  print(\"Original data samples\", org_train_np.shape[0])\n",
        "  print(\"Total Upsampled examples\", counterex_robust.shape[0]+upsample_counterex_robust.shape[0]+upsample_counterex_clus.shape[0]+counterex_clus.shape[0])\n",
        "  \n",
        "  # Create a data frame of Upsampled data + original data\n",
        "  df_up = pd.DataFrame(np.vstack((counterex_robust, counterex_clus, upsample_counterex_robust, upsample_counterex_clus, org_train_np)))\n",
        "\n",
        "  # Rename the columns with feature names\n",
        "  df_up.columns = ['FixationDuration', 'FixationSeq', 'FixationStart', \n",
        "                'FixationX', 'FixationY', 'GazeDirectionLeftZ', 'GazeDirectionRightZ', \n",
        "                'PupilLeft', 'PupilRight', 'InterpolatedGazeX', 'InterpolatedGazeY', \n",
        "                'AutoThrottle', 'AutoWheel', 'CurrentThrottle', 'CurrentWheel', \n",
        "                'Distance3D', 'MPH', 'ManualBrake', 'ManualThrottle', 'ManualWheel', \n",
        "                'RangeW', 'RightLaneDist', 'RightLaneType', 'LeftLaneDist', 'LeftLaneType', \n",
        "                'TOT_fast', 'TOT_med_fast', 'TOT_med', 'TOT_med_slow', 'TOT_slow']\n",
        "\n",
        "  # Convert df to csv and save\n",
        "  df_up.to_csv('counter_robust_clus_dup.csv')\n",
        "\n",
        "  print(\"~ Preprocessing done\")\n",
        "  print(\"-\"*30)\n",
        "\n",
        "  print(\"~ Training:\")\n",
        "  x = df_up.iloc[:,:25] # features\n",
        "  y = df_up.iloc[:,25:] # targets\n",
        "\n",
        "  # Split data | 80% (for training)- 20% (testing)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=27, shuffle=False)\n",
        "\n",
        "  # Train\n",
        "  history = train(X_train, X_test, y_train, y_test)\n",
        "\n",
        "  print(\"~ Done\")\n",
        "  print(\"-\"*30)\n",
        "\n",
        "  # Plot loss and accuracy graphs\n",
        "  print(\"~ Plotting loss and accuracy\")\n",
        "  plot_acc(history)\n",
        "  plot_loss(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}